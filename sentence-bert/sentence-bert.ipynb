{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitsentencebertconda5e290f8b89784fb487a9a3fce8d56951",
   "display_name": "Python 3.7.3 64-bit ('sentence-bert': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence BERT\n",
    "The purpose of this notebook is to experiment with the functionallity Sentence BERT brings in the context of semantic search and clustering. \n",
    "\n",
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def timer(f, *args, **kwargs):\n",
    "    start = time.time()\n",
    "    out = f(*args, **kwargs)\n",
    "    print(f\"Took {round(time.time() - start , 3)} to execute {f.__name__}\")\n",
    "    return out\n",
    "#end def\n",
    "\n",
    "class SimMeasurer():\n",
    "    def __init__(self, text: list, model: SentenceTransformer, embedded_text=[]):\n",
    "        self.text = text\n",
    "        self.model = model\n",
    "\n",
    "        if not embedded_text:\n",
    "            self.embedded_text = model.encode(text)\n",
    "        else:\n",
    "            self.embedded_text = embedded_text\n",
    "        #end if\n",
    "\n",
    "        self.precomputed_norm = norm(self.embedded_text, axis=1)\n",
    "    #end def\n",
    "\n",
    "    def matrix_cosine_sim(self, query):\n",
    "        # Query needs to be 2d row vector\n",
    "        query_norm = norm(query)        \n",
    "\n",
    "        prod = np.ndarray.flatten(np.matmul(self.embedded_text, query))\n",
    "        #vector_norm = np.ndarray.flatten((query_norm * self.precomputed_norm))\n",
    "\n",
    "        cosine_sim = np.divide(prod, query_norm * self.precomputed_norm)\n",
    "        return cosine_sim\n",
    "    #end def\n",
    "\n",
    "    def get_n_most_similar(self, query, n=1):\n",
    "        similarity_scores = self.matrix_cosine_sim(query)\n",
    "\n",
    "        top_idx = []\n",
    "        top_scores = []\n",
    "        for i in range(n):\n",
    "            idx = np.argmax(similarity_scores)\n",
    "            score = similarity_scores[idx]\n",
    "\n",
    "            top_idx.append(idx)\n",
    "            top_scores.append(score)\n",
    "\n",
    "            similarity_scores[idx] = 0\n",
    "        #end for\n",
    "\n",
    "        return top_idx, top_scores\n",
    "    #end def\n",
    "\n",
    "    def get_most_similar(self, query, n=3, scores=False):\n",
    "        start = time.time()\n",
    "        query_vector = np.reshape(model.encode([query]), (-1,1))\n",
    "        top_idx, scores = self.get_n_most_similar(query_vector, n)\n",
    "        print(f'Took {round(time.time()-start, 3)} s to encode query and find {n} most similar sentences')\n",
    "\n",
    "        if scores:\n",
    "            return [self.text[t] for t in top_idx], scores\n",
    "        else:\n",
    "            return [self.text[t] for t in top_idx]\n",
    "    #end def\n",
    "\n",
    "#end class\n",
    "\n",
    "  \n",
    "class QuoraDataset():\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.dict_data = self._read_from_file()\n",
    "\n",
    "        self.len = len(self.dict_data)\n",
    "    #end def\n",
    "\n",
    "    def _read_from_file(self):\n",
    "        with open(self.file_path, 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            dict_data = [row for row in reader]\n",
    "\n",
    "        return dict_data\n",
    "    #end def\n",
    "\n",
    "    def get_questions(self, n=1, deduplicate=True):\n",
    "        questions = set()\n",
    "        for row in self.dict_data:\n",
    "            for q_idx in [1,2]:\n",
    "                questions.add(row.get(f'question{q_idx}'))\n",
    "                if len(questions) >= n:\n",
    "                    return list(questions)\n",
    "                #end if\n",
    "                \n",
    "                if row.get('is_duplicate') == '1':\n",
    "                    break\n",
    "                #end if\n",
    "            #end for\n",
    "        #end for\n",
    "    #end def\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    #end def\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"I am a collection of {self.len} questions from path {self.file_path}\"\n",
    "    #end def\n",
    "\n",
    "\n",
    "#end class     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|██████████| 405M/405M [00:47<00:00, 8.47MB/s]\nTook 52.265 to execute SentenceTransformer\nTook 2.807 to execute QuoraDataset\nI am a collection of 404290 questions from path train.csv\n"
    }
   ],
   "source": [
    "model = timer(SentenceTransformer, 'bert-base-nli-stsb-mean-tokens')\n",
    "dataset = timer(QuoraDataset, 'train.csv')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract questions from dataset and encode them using the sentnece bert model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Took 0.001 to execute get_questions\nTook 19.124 to execute encode\n"
    }
   ],
   "source": [
    "questions = timer(dataset.get_questions, n=1000)\n",
    "encoded_questions = timer(model.encode, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the similarity between a query and the embedded questions from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Took 0.041 s to encode query and find 10 most similar sentences\nHow do i learn faster?\n------------------\nScore: 0.544 - How we can travel faster than light?                            \nScore: 0.533 - How do I manage time for studies?                               \nScore: 0.53 - How do I start writing again?                                   \nScore: 0.52 - How do I keep motivation to learn a language?                   \nScore: 0.508 - How can I become a good speaker?                                \nScore: 0.492 - How do I post a question that was marked as needing improvement?\nScore: 0.491 - What is the alternative to machine learning?                    \nScore: 0.475 - How do I start writing?                                         \nScore: 0.469 - How can one learn to trust again?                               \nScore: 0.463 - How much funds should I raise?                                  \n"
    }
   ],
   "source": [
    "sim = SimMeasurer(questions, model, encoded_questions)\n",
    "\n",
    "query = 'How do i learn faster?'\n",
    "most_sim = sim.get_most_similar(query, n=10)\n",
    "\n",
    "print(query + \"\\n------------------\")\n",
    "offset = len(max(most_sim[0], key=len))\n",
    "for t, s in zip(most_sim[0], most_sim[1]):\n",
    "    print(\"Score: {:.3} - {:<{offset}}\".format(s, t, offset=offset))\n",
    "    #print(f\"{t} \\t\\t (Score: {round(s, 3)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Extracted 47 values for key Description from dataset\n"
    }
   ],
   "source": [
    "import re\n",
    "text = extract_key(\"Description\")\n",
    "\n",
    "sentence_pattern = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s')\n",
    "newline_pattern = re.compile(r'\\s+')\n",
    "sentences = [re.split(sentence_pattern, t) for t in text]\n",
    "#sentences = [re.sub(newline_pattern, ' ', s) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Quora dataset\n",
    "This dataset is perfect for a couple of reasons:\n",
    "    1. It ties in to the report i wrote previously\n",
    "    2. It has clean sentences that I can embed\n",
    "    3. I can in a later iteration fine-tune the sentence bert model on this data which will allow me to dig futrther into the architecture of the model etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n"
    }
   ],
   "source": [
    "from random import sample\n",
    "from random import seed\n",
    "seed(42)\n",
    "import csv\n",
    "with open('train.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    print(header)\n",
    "    data = [r for r in reader]\n",
    "\n",
    "\n",
    "dev_size = round(0.1*len(labels))\n",
    "\n",
    "dev_data_idx = set(sample(range(len(data)), dev_size))\n",
    "\n",
    "with open('quora-train.csv', 'w') as train, open('quora-dev.csv', 'w') as dev:\n",
    "    train_writer = csv.writer(train)\n",
    "    dev_writer = csv.writer(dev)\n",
    "    for i, row in enumerate(data):\n",
    "        if i in dev_data_idx:\n",
    "            dev_writer.writerow(row)\n",
    "        else:\n",
    "            train_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuneing SentenceBERT on this task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}