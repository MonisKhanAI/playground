{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /Users/Viktor/miniconda3/envs/transformers/lib/python3.6/site-packages (0.7.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                            CharBPETokenizer,\n",
    "                            SentencePieceBPETokenizer,\n",
    "                            BertWordPieceTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bbcsport_train.csv\") as f:\n",
    "    reader = csv.reader(f, delimiter=\";\")\n",
    "    data = [] \n",
    "    for row in reader:\n",
    "        data.append(row[1])\n",
    "\n",
    "with open(\"bbcsport_train.txt\", \"w\") as out:\n",
    "    out.write(\"\\n\\n\".join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "test_text = \"\"\"Sri Lankans cleared of misconduct. Då är öarna döda sportåäö. Two Sri Lanka cricketers have been cleared of misconduct dating back to the ICC Champions Trophy in 2004.  Avishka Gunawardene and Kaushal Lokuarachchi were both the subject of an official disciplinary inquiry after allegations of drunken misconduct. A Colombo newspaper had made the claims after a defeat against England in Southampton which led to Sri Lanka exiting the tournament early. But the disciplinary panel could find no evidence against the players. Sri Lanka Cricket chief executive Duleep Mendis said: \"Nobody was prepared to give evidence and there was absolutely no evidence to substantiate the article's allegations.\" Gunawardene, 27, a hard-hitting opener, and Lokuarachchi, a 22-year-old leg-spinning all-rounder, were both dropped from the national squad after Sri Lanka's tour of Pakistan in October.\"\"\"\n",
    "for t in (ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer, BertWordPieceTokenizer):\n",
    "    tokenizer = t()\n",
    "    tokenizer.train([\"bbcsport_train.txt\"], vocab_size=5000)\n",
    "    out = tokenizer.encode(test_text)\n",
    "    tokens.append(out.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sri Lankans cleared of misconduct. Då är öarna döda sportåäö. Two Sri Lanka cricketers have been cleared of misconduct dating back to the ICC Champions Trophy in 2004.  Avishka Gunawardene and Kaushal Lokuarachchi were both the subject of an official disciplinary inquiry after allegations of drunken misconduct. A Colombo newspaper had made the claims after a defeat against England in Southampton which led to Sri Lanka exiting the tournament early. But the disciplinary panel could find no evidence against the players. Sri Lanka Cricket chief executive Duleep Mendis said: \"Nobody was prepared to give evidence and there was absolutely no evidence to substantiate the article's allegations.\" Gunawardene, 27, a hard-hitting opener, and Lokuarachchi, a 22-year-old leg-spinning all-rounder, were both dropped from the national squad after Sri Lanka's tour of Pakistan in October.\n",
      "Sri ĠLankans Ġcleared Ġof Ġmisconduct . ĠD Ã ¥ Ġ Ã ¤ r Ġ Ã ¶ ar na Ġd Ã ¶ d a Ġsport Ã ¥ Ã ¤ Ã ¶ . ĠTwo ĠSri ĠLanka Ġcricketers Ġhave Ġbeen Ġcleared Ġof Ġmisconduct Ġd ating Ġback Ġto Ġthe ĠICC ĠChampions ĠTrophy Ġin Ġ2004 . Ġ ĠA v ish ka ĠGun awardene Ġand ĠKa us hal ĠL ok u ar ach chi Ġwere Ġboth Ġthe Ġsubject Ġof Ġan Ġofficial Ġdisciplinary Ġinquiry Ġafter Ġallegations Ġof Ġdr un ken Ġmisconduct . ĠA ĠCol omb o Ġnewspaper Ġhad Ġmade Ġthe Ġclaims Ġafter Ġa Ġdefeat Ġagainst ĠEngland Ġin ĠSouthampton Ġwhich Ġled Ġto ĠSri ĠLanka Ġex iting Ġthe Ġtournament Ġearly . ĠBut Ġthe Ġdisciplinary Ġpanel Ġcould Ġfind Ġno Ġevidence Ġagainst Ġthe Ġplayers . ĠSri ĠLanka ĠCricket Ġchief Ġexecutive ĠDu le ep ĠMend is Ġsaid : Ġ\" No body Ġwas Ġprepared Ġto Ġgive Ġevidence Ġand Ġthere Ġwas Ġabsolutely Ġno Ġevidence Ġto Ġsubst ant i ate Ġthe Ġarticle 's Ġallegations .\" ĠGun awardene , Ġ27 , Ġa Ġhard - h itting Ġopener , Ġand ĠL ok u ar ach chi , Ġa Ġ22 - year - old Ġleg - sp inning Ġall - rounder , Ġwere Ġboth Ġdropped Ġfrom Ġthe Ġnational Ġsquad Ġafter ĠSri ĠLanka 's Ġtour Ġof ĠPakistan Ġin ĠOctober .\n",
      "---\n",
      "Sri</w> Lankans</w> cleared</w> of</w> misconduct</w> .</w> D a</w> ar</w> o ar na</w> do da</w> sp or ta a o</w> .</w> Two</w> Sri</w> Lanka</w> cricketers</w> have</w> been</w> cleared</w> of</w> misconduct</w> d ating</w> back</w> to</w> the</w> ICC</w> Champions</w> Trophy</w> in</w> 2004</w> .</w> A v ish ka</w> Gun a wardene</w> and</w> K au sha l</w> Lo ku ar ach chi</w> were</w> both</w> the</w> subject</w> of</w> an</w> official</w> disciplinary</w> inquiry</w> after</w> allegations</w> of</w> d run ken</w> misconduct</w> .</w> A</w> Co lo mb o</w> newspaper</w> had</w> made</w> the</w> claims</w> after</w> a</w> defeat</w> against</w> England</w> in</w> Southampton</w> which</w> led</w> to</w> Sri</w> Lanka</w> ex iting</w> the</w> tournament</w> early</w> .</w> But</w> the</w> disciplinary</w> panel</w> could</w> find</w> no</w> evidence</w> against</w> the</w> players</w> .</w> Sri</w> Lanka</w> Cricket</w> chief</w> executive</w> Du le ep</w> Mend is</w> said</w> :</w> \"</w> No body</w> was</w> prepared</w> to</w> give</w> evidence</w> and</w> there</w> was</w> absolutely</w> no</w> evidence</w> to</w> substanti ate</w> the</w> article</w> '</w> s</w> allegations</w> .</w> \"</w> Gun a wardene</w> ,</w> 27</w> ,</w> a</w> hard</w> -</w> hitting</w> opener</w> ,</w> and</w> Lo ku ar ach chi</w> ,</w> a</w> 22</w> -</w> year</w> -</w> old</w> leg</w> -</w> spinn ing</w> all</w> -</w> rounder</w> ,</w> were</w> both</w> dropped</w> from</w> the</w> national</w> squad</w> after</w> Sri</w> Lanka</w> '</w> s</w> tour</w> of</w> Pakistan</w> in</w> October</w> .</w>\n",
      "---\n",
      "▁Sri ▁Lankans ▁cleared ▁of ▁miscond uct . ▁D ▁r ▁ar na ▁d d a ▁sport. ▁Two ▁Sri ▁Lanka ▁cricketers ▁have ▁been ▁cleared ▁of ▁miscond uct ▁d ating ▁back ▁to ▁the ▁ICC ▁Champions ▁Trophy ▁in ▁2004. ▁ ▁A v ish ka ▁Gun award ene ▁and ▁Ka us hal ▁L ok u ar ach ch i ▁were ▁both ▁the ▁subject ▁of ▁an ▁official ▁disciplinary ▁inquiry ▁after ▁allegations ▁of ▁dr un ken ▁miscond uct . ▁A ▁Col omb o ▁newspaper ▁had ▁made ▁the ▁claims ▁after ▁a ▁defeat ▁against ▁England ▁in ▁Southampton ▁which ▁led ▁to ▁Sri ▁Lanka ▁ex iting ▁the ▁tournament ▁early. ▁But ▁the ▁disciplinary ▁panel ▁could ▁find ▁no ▁evidence ▁against ▁the ▁players. ▁Sri ▁Lanka ▁Cricket ▁chief ▁executive ▁Du le ep ▁Mend is ▁said: ▁\"No body ▁was ▁prepared ▁to ▁give ▁evidence ▁and ▁there ▁was ▁absolutely ▁no ▁evidence ▁to ▁subst ant i ate ▁the ▁artic le 's ▁allegations .\" ▁Gun award en e, ▁27, ▁a ▁hard -h itting ▁open er, ▁and ▁L ok u ar ach chi, ▁a ▁22-year-old ▁leg -sp inning ▁all-rounder , ▁were ▁both ▁dropped ▁from ▁the ▁national ▁squad ▁after ▁Sri ▁Lanka 's ▁tour ▁of ▁Pakistan ▁in ▁October.\n",
      "---\n",
      "sri lankans cleared of misconduct . da ar o ##ar ##na do ##d ##a sport ##a ##a ##o . two sri lanka cricketers have been cleared of misconduct dating back to the icc champions trophy in 2004 . a ##vis ##h ##ka gun ##awardene and ka ##ush ##al lo ##ku ##arach ##chi were both the subject of an official disciplinary inquiry after allegations of dr ##un ##ken misconduct . a col ##omb ##o newspaper had made the claims after a defeat against england in southampton which led to sri lanka exit ##ing the tournament early . but the disciplinary panel could find no evidence against the players . sri lanka cricket chief executive du ##leep mend ##is said : \" nobody was prepared to give evidence and there was absolutely no evidence to subst ##anti ##ate the article ' s allegations . \" gun ##awardene , 27 , a hard - hitting opener , and lo ##ku ##arach ##chi , a 22 - year - old leg - spin ##ning all - rounder , were both dropped from the national squad after sri lanka ' s tour of pakistan in october .\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(test_text)\n",
    "for t in tokens:\n",
    "    print(\" \".join(t))\n",
    "    #print([i.encode('utf-8') for i in t])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE\n",
    "BPE or byte pair encoding stemms from a compression algorithm created in 1994. Its purpose was there to compress text by replacing the most frequent substrings with special characters. Repeting this process over multiple passes enables units of toknes to be created that in turn build the vocabulary.\n",
    "\n",
    "Instead of replacing the most common units with special characters in order to compress the text is it possible to use the same technique to build a vocabulary. \n",
    "\n",
    "Run the same algorithm, but instead of replacing the most common token units, consider it a part of your vocabulary. \n",
    "# Split by space and call it a day?\n",
    " - Discuss shortcommings of this simple method\n",
    "     - Language specific\n",
    "         - Normalisation\n",
    " - Mention vocab size of word2vec and similar and how that is not really viable (cannot deal with rare words)\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Character level BPE\n",
    "Source: https://www.aclweb.org/anthology/P16-1162.pdf\n",
    "\n",
    "Each word is initially segmented into its characters and ended with a special </w> token to indicate the end of a word. BPE is then run to create subword units from the most common ones\n",
    "\n",
    "\n",
    "## Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=0, model=BPE, unk_token=<unk>, suffix=</w>, dropout=None, lowercase=False, unicode_normalizer=None, bert_normalizer=True, split_on_whitespace_only=False)\n",
      "\n",
      "Vocab size - Encoded test string\n",
      "       100 - l o w</w> l o w e r</w> l o w e s t</w> n e w e r</w> w i d e r</w> f i n d e r</w>\n",
      "       250 - lo w</w> lo w er</w> lo w e st</w> ne w er</w> w i d er</w> f in d er</w>\n",
      "       500 - lo w</w> lo w er</w> lo w est</w> ne w er</w> w i der</w> fin der</w>\n",
      "      1000 - low</w> low er</w> low est</w> ne w er</w> wi der</w> fin der</w>\n"
     ]
    }
   ],
   "source": [
    "s = \"low lower lowest newer wider finder\"\n",
    "with open('test_string.txt', 'w') as out:\n",
    "    out.write(s)\n",
    "tokeniser = CharBPETokenizer()\n",
    "print(tokeniser)\n",
    "\n",
    "print(f\"\\n{'Vocab size':<10} - {'Encoded test string'}\")\n",
    "for vocab_size in [100, 250, 500, 1000]:\n",
    "    tokeniser.train(['bbcsport_train.txt'], vocab_size=vocab_size)\n",
    "    encoded = tokeniser.encode(s)\n",
    "    print(f\"{vocab_size:>10} - {' '.join(encoded.tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on CharLevel BPE\n",
    "- Not entierly lossless encoding. \n",
    "- Requires language specific segmentation for preprocessing. \n",
    "\n",
    "Q: Is there an initial set of characters or does it adapt to the dataset at hand?\n",
    "A: Seems like it does not encode åäö in any particularly good way... (simply removes the dots from these characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Byte level\n",
    "Character level struggles with character-rich languages (Chinese, Japanese) due to the much larger vocabulary. This often leads to unecessarily large vocabularies which slows down processing. Byte level tokenisation addresses this issue through initially represent each character as a set of bytes, which can vary between 1 and 4. The set of 256 bytes used to represent all characters includes the ascii set, which mean that most european language characters can be represented in their original form.\n",
    "\n",
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=0, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)\n",
      "\n",
      "Vocab size - Encoded test string\n",
      "       100 - Ġ S w Ã © d i s h Ġ e x a m p l e : Ġ n Ã ¤ r Ġ Ã ¥ t Ġ Ã ĸ r j a n ? Ġ ( W h e n Ġ d i d Ġ O r j a n Ġ e a t ? )\n",
      "       500 - ĠS w Ã © d is h Ġex am p le : Ġn Ã ¤ r Ġ Ã ¥ t Ġ Ã ĸ r j an ? Ġ( W hen Ġd id ĠO r j an Ġe at ? )\n",
      "      1000 - ĠS w Ã © d ish Ġex amp le : Ġn Ã ¤ r Ġ Ã ¥ t Ġ Ã ĸ r j an ? Ġ( W hen Ġdid ĠO r j an Ġe at ? )\n"
     ]
    }
   ],
   "source": [
    "s = \" Swédish example: när åt Örjan? (When did Orjan eat?)\"\n",
    "\n",
    "test_file_name = \"test_text_with_swedish_characters.txt\"\n",
    "with open(test_file_name, \"w\") as out:\n",
    "    out.write(s)\n",
    "\n",
    "tokeniser = ByteLevelBPETokenizer()\n",
    "print(tokeniser)\n",
    "print(f\"\\n{'Vocab size':<10} - {'Encoded test string'}\")\n",
    "for vocab_size in [100, 500, 1000]:\n",
    "    tokeniser.train([\"bbcsport_train.txt\"], vocab_size=vocab_size)\n",
    "    encoded = tokeniser.encode(s)\n",
    "    print(f\"{vocab_size:>10} - {' '.join(encoded.tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc3\\xa9'\n",
      "b'\\xc3\\xa4'\n",
      "b'\\xc3\\x83 \\xc2\\xa4'\n"
     ]
    }
   ],
   "source": [
    "print(\"é\".encode('utf-8'))\n",
    "print(\"ä\".encode(\"utf-8\"))\n",
    "print(\"Ã ¤\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Swédish example: när åt Örjan? (When did Orjan eat?)'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.decode(encoded.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the output above looks wiered, there is structure too it. Whitespace characters are replaced by the special \"Ġ\" character. Its byte representation can be found through the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "This new sequence of \"characters\" is then processed through the BPE algorithm in order to create tokens of the most common byte-level \"subword\" units. The original paper gives a great example. They show the learnt tokens with increasing allowed vocabulary size. As this increases, more high level representations (often almost entire words) are stored while at the lowest level the representations are more raw (closer to the original token representation). \n",
    "\n",
    "Resources: https://arxiv.org/pdf/1909.03341.pdf, https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "### Questions for ByteLevel encodings\n",
    "Does not seem like the original byte represenattion is used. Encoding \"å\" with utf-8 results in a different set of bytes compared to what is used for the BPE process. Why is that? What has happend?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SentencePiece\n",
    "Adresses some issues with previous tokenisers\n",
    "1. Requires language specific pretokenisation processing. I think this basically means that we need to know the word units before the new vocabulary can be built from subwords. For european languages is this often possible to generate simply by splitting on spaces etc. It is however not perfect and requires extra work, especially when moving over to non-european languages such as Chinese, Japanese etc, where whitespace is not used to indicate new words.\n",
    "2. Tokenisation is not reversable. I.e. cannot go from tokenised text back to original one without ambiguity\n",
    "3. Reproducability of tokenisers described in resarch. \n",
    "\n",
    "Implemented through BPE \n",
    "Utilise standard for character normalisation defined by the Unicode standard Normalisation Form (e.g NFC and NFKC)\n",
    "## Unicode Standard Normalisation Form\n",
    "Canonically equivalent or compatiable\n",
    "First should be displayed and treated in the same way while the second should be ok to replace with its compatiable counterpart.\n",
    "> Compatible sequences may be treated the same way in some applications (such as sorting and indexing), but not in others; and may be substituted for each other in some situations, but not in others. Sequences that are canonically equivalent are also compatible, but the opposite is not necessarily true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=0, model=SentencePieceBPE, unk_token=<unk>, replacement=▁, add_prefix_space=True, dropout=None)\n",
      "\n",
      "Vocab size - Encoded test string\n",
      "       100 - ▁ l o w er ▁ l o w e s t ▁ n e w er ▁f in d er , ▁ ▁ e n t r c t e\n",
      "       250 - ▁l ow er ▁l ow est ▁n e w er ▁f in d er , ▁ ▁ ent r c t e\n",
      "       500 - ▁l ow er ▁l ow est ▁ne w er ▁fin d er , ▁ ▁ ent r ct e\n",
      "      1000 - ▁l ow er ▁l ow est ▁new er ▁fin d er, ▁ ▁ ent r ct e\n",
      "      5000 - ▁lower ▁low est ▁new er ▁find er, ▁ ▁ent r ct e\n",
      "lower lowest newer finder,  entrcte\n"
     ]
    }
   ],
   "source": [
    "s = \"lower lowest newer finder, åäö entrécôte\"\n",
    "with open('test_string.txt', 'w') as out:\n",
    "    out.write(s)\n",
    "tokeniser = SentencePieceBPETokenizer()\n",
    "print(tokeniser)\n",
    "\n",
    "print(f\"\\n{'Vocab size':<10} - {'Encoded test string'}\")\n",
    "for vocab_size in [100, 250, 500, 1000, 5000]:\n",
    "    tokeniser.train(['bbcsport_train.txt'], vocab_size=vocab_size)\n",
    "    encoded = tokeniser.encode(s)\n",
    "    print(f\"{vocab_size:>10} - {' '.join(encoded.tokens)}\")\n",
    "print(tokeniser.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## WordPiece\n",
    "\n",
    "Rsources: https://arxiv.org/pdf/1609.08144.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "('ğ', b'\\xc4\\x9f')\n",
      "('^', b'^')\n",
      "('Z', b'Z')\n",
      "('â', b'\\xc3\\xa2')\n",
      "('Ö', b'\\xc3\\x96')\n",
      "('8', b'8')\n",
      "('q', b'q')\n",
      "('Ć', b'\\xc4\\x86')\n",
      "('c', b'c')\n",
      "('Ä', b'\\xc3\\x84')\n",
      "('¹', b'\\xc2\\xb9')\n",
      "('ć', b'\\xc4\\x87')\n",
      "('Ë', b'\\xc3\\x8b')\n",
      "('V', b'V')\n",
      "('T', b'T')\n",
      "('Ì', b'\\xc3\\x8c')\n",
      "('G', b'G')\n",
      "('\"', b'\"')\n",
      "('¸', b'\\xc2\\xb8')\n",
      "('Õ', b'\\xc3\\x95')\n",
      "('Ó', b'\\xc3\\x93')\n",
      "('´', b'\\xc2\\xb4')\n",
      "('ķ', b'\\xc4\\xb7')\n",
      "('Ç', b'\\xc3\\x87')\n",
      "('\\\\', b'\\\\')\n",
      "('ĝ', b'\\xc4\\x9d')\n",
      "('Ė', b'\\xc4\\x96')\n",
      "('Ù', b'\\xc3\\x99')\n",
      "('ĸ', b'\\xc4\\xb8')\n",
      "('y', b'y')\n",
      "('ü', b'\\xc3\\xbc')\n",
      "('¡', b'\\xc2\\xa1')\n",
      "('Æ', b'\\xc3\\x86')\n",
      "('l', b'l')\n",
      "('%', b'%')\n",
      "('¾', b'\\xc2\\xbe')\n",
      "('ħ', b'\\xc4\\xa7')\n",
      "('ĩ', b'\\xc4\\xa9')\n",
      "('Ł', b'\\xc5\\x81')\n",
      "('Ĵ', b'\\xc4\\xb4')\n",
      "('¬', b'\\xc2\\xac')\n",
      "('«', b'\\xc2\\xab')\n",
      "('ī', b'\\xc4\\xab')\n",
      "('Ċ', b'\\xc4\\x8a')\n",
      "('ď', b'\\xc4\\x8f')\n",
      "('Ļ', b'\\xc4\\xbb')\n",
      "('ł', b'\\xc5\\x82')\n",
      "('.', b'.')\n",
      "('S', b'S')\n",
      "('_', b'_')\n",
      "('(', b'(')\n",
      "('U', b'U')\n",
      "('ô', b'\\xc3\\xb4')\n",
      "('Ń', b'\\xc5\\x83')\n",
      "('æ', b'\\xc3\\xa6')\n",
      "('ª', b'\\xc2\\xaa')\n",
      "('W', b'W')\n",
      "('=', b'=')\n",
      "('ċ', b'\\xc4\\x8b')\n",
      "('k', b'k')\n",
      "('Č', b'\\xc4\\x8c')\n",
      "('Ģ', b'\\xc4\\xa2')\n",
      "('$', b'$')\n",
      "('N', b'N')\n",
      "('t', b't')\n",
      "('Þ', b'\\xc3\\x9e')\n",
      "('r', b'r')\n",
      "('í', b'\\xc3\\xad')\n",
      "('Ð', b'\\xc3\\x90')\n",
      "('?', b'?')\n",
      "('&', b'&')\n",
      "('Ľ', b'\\xc4\\xbd')\n",
      "('n', b'n')\n",
      "('£', b'\\xc2\\xa3')\n",
      "('Å', b'\\xc3\\x85')\n",
      "('x', b'x')\n",
      "('-', b'-')\n",
      "('Â', b'\\xc3\\x82')\n",
      "('z', b'z')\n",
      "('·', b'\\xc2\\xb7')\n",
      "('÷', b'\\xc3\\xb7')\n",
      "('ĺ', b'\\xc4\\xba')\n",
      "('ė', b'\\xc4\\x97')\n",
      "('Ï', b'\\xc3\\x8f')\n",
      "('Ĳ', b'\\xc4\\xb2')\n",
      "('Ý', b'\\xc3\\x9d')\n",
      "('u', b'u')\n",
      "('ó', b'\\xc3\\xb3')\n",
      "('º', b'\\xc2\\xba')\n",
      "(')', b')')\n",
      "('ļ', b'\\xc4\\xbc')\n",
      "('Ĩ', b'\\xc4\\xa8')\n",
      "('Ĉ', b'\\xc4\\x88')\n",
      "('ı', b'\\xc4\\xb1')\n",
      "('Ġ', b'\\xc4\\xa0')\n",
      "('a', b'a')\n",
      "('O', b'O')\n",
      "('A', b'A')\n",
      "('Ā', b'\\xc4\\x80')\n",
      "('Į', b'\\xc4\\xae')\n",
      "('É', b'\\xc3\\x89')\n",
      "('D', b'D')\n",
      "('H', b'H')\n",
      "('¶', b'\\xc2\\xb6')\n",
      "('ľ', b'\\xc4\\xbe')\n",
      "('5', b'5')\n",
      "('²', b'\\xc2\\xb2')\n",
      "('Ě', b'\\xc4\\x9a')\n",
      "('Ú', b'\\xc3\\x9a')\n",
      "('>', b'>')\n",
      "('Ď', b'\\xc4\\x8e')\n",
      "('b', b'b')\n",
      "('ö', b'\\xc3\\xb6')\n",
      "('M', b'M')\n",
      "('~', b'~')\n",
      "('/', b'/')\n",
      "('Ĝ', b'\\xc4\\x9c')\n",
      "('Ŀ', b'\\xc4\\xbf')\n",
      "('©', b'\\xc2\\xa9')\n",
      "('e', b'e')\n",
      "('6', b'6')\n",
      "('C', b'C')\n",
      "('ġ', b'\\xc4\\xa1')\n",
      "('<', b'<')\n",
      "('ß', b'\\xc3\\x9f')\n",
      "('¤', b'\\xc2\\xa4')\n",
      "('ĕ', b'\\xc4\\x95')\n",
      "('¨', b'\\xc2\\xa8')\n",
      "('á', b'\\xc3\\xa1')\n",
      "('Ê', b'\\xc3\\x8a')\n",
      "('!', b'!')\n",
      "('ĥ', b'\\xc4\\xa5')\n",
      "('`', b'`')\n",
      "('ë', b'\\xc3\\xab')\n",
      "('p', b'p')\n",
      "('ĉ', b'\\xc4\\x89')\n",
      "('ñ', b'\\xc3\\xb1')\n",
      "('Ī', b'\\xc4\\xaa')\n",
      "('ě', b'\\xc4\\x9b')\n",
      "('ă', b'\\xc4\\x83')\n",
      "('Ñ', b'\\xc3\\x91')\n",
      "('ĵ', b'\\xc4\\xb5')\n",
      "('ï', b'\\xc3\\xaf')\n",
      "('Đ', b'\\xc4\\x90')\n",
      "('å', b'\\xc3\\xa5')\n",
      "('İ', b'\\xc4\\xb0')\n",
      "('ŀ', b'\\xc5\\x80')\n",
      "('g', b'g')\n",
      "('§', b'\\xc2\\xa7')\n",
      "('½', b'\\xc2\\xbd')\n",
      "('Á', b'\\xc3\\x81')\n",
      "('³', b'\\xc2\\xb3')\n",
      "('î', b'\\xc3\\xae')\n",
      "('[', b'[')\n",
      "('Ğ', b'\\xc4\\x9e')\n",
      "('À', b'\\xc3\\x80')\n",
      "('ã', b'\\xc3\\xa3')\n",
      "('Ã', b'\\xc3\\x83')\n",
      "('þ', b'\\xc3\\xbe')\n",
      "('w', b'w')\n",
      "('é', b'\\xc3\\xa9')\n",
      "('9', b'9')\n",
      "(\"'\", b\"'\")\n",
      "('ę', b'\\xc4\\x99')\n",
      "('{', b'{')\n",
      "('ä', b'\\xc3\\xa4')\n",
      "('Ø', b'\\xc3\\x98')\n",
      "('Q', b'Q')\n",
      "('¿', b'\\xc2\\xbf')\n",
      "('±', b'\\xc2\\xb1')\n",
      "('È', b'\\xc3\\x88')\n",
      "('j', b'j')\n",
      "('ò', b'\\xc3\\xb2')\n",
      "('1', b'1')\n",
      "('}', b'}')\n",
      "('Ă', b'\\xc4\\x82')\n",
      "('B', b'B')\n",
      "('Ĭ', b'\\xc4\\xac')\n",
      "('o', b'o')\n",
      "('ð', b'\\xc3\\xb0')\n",
      "('Ķ', b'\\xc4\\xb6')\n",
      "('ą', b'\\xc4\\x85')\n",
      "('0', b'0')\n",
      "('õ', b'\\xc3\\xb5')\n",
      "('m', b'm')\n",
      "('¯', b'\\xc2\\xaf')\n",
      "('Ę', b'\\xc4\\x98')\n",
      "('ú', b'\\xc3\\xba')\n",
      "('ç', b'\\xc3\\xa7')\n",
      "('ā', b'\\xc4\\x81')\n",
      "('è', b'\\xc3\\xa8')\n",
      "('¥', b'\\xc2\\xa5')\n",
      "('Î', b'\\xc3\\x8e')\n",
      "('Ĺ', b'\\xc4\\xb9')\n",
      "('à', b'\\xc3\\xa0')\n",
      "('E', b'E')\n",
      "('×', b'\\xc3\\x97')\n",
      "(']', b']')\n",
      "('ø', b'\\xc3\\xb8')\n",
      "('s', b's')\n",
      "('X', b'X')\n",
      "('ģ', b'\\xc4\\xa3')\n",
      "('ĭ', b'\\xc4\\xad')\n",
      "('Ô', b'\\xc3\\x94')\n",
      "('F', b'F')\n",
      "('»', b'\\xc2\\xbb')\n",
      "('|', b'|')\n",
      "('ĳ', b'\\xc4\\xb3')\n",
      "('°', b'\\xc2\\xb0')\n",
      "('č', b'\\xc4\\x8d')\n",
      "('đ', b'\\xc4\\x91')\n",
      "('Ą', b'\\xc4\\x84')\n",
      "('ÿ', b'\\xc3\\xbf')\n",
      "('h', b'h')\n",
      "(':', b':')\n",
      "('3', b'3')\n",
      "('ì', b'\\xc3\\xac')\n",
      "('P', b'P')\n",
      "('J', b'J')\n",
      "('v', b'v')\n",
      "('ê', b'\\xc3\\xaa')\n",
      "(',', b',')\n",
      "('i', b'i')\n",
      "('Ò', b'\\xc3\\x92')\n",
      "('Í', b'\\xc3\\x8d')\n",
      "('4', b'4')\n",
      "('7', b'7')\n",
      "('Ĕ', b'\\xc4\\x94')\n",
      "('Ĥ', b'\\xc4\\xa4')\n",
      "('R', b'R')\n",
      "('#', b'#')\n",
      "('Û', b'\\xc3\\x9b')\n",
      "('L', b'L')\n",
      "('K', b'K')\n",
      "('Y', b'Y')\n",
      "('ē', b'\\xc4\\x93')\n",
      "('¼', b'\\xc2\\xbc')\n",
      "('®', b'\\xc2\\xae')\n",
      "('Ħ', b'\\xc4\\xa6')\n",
      "('@', b'@')\n",
      "('¢', b'\\xc2\\xa2')\n",
      "(';', b';')\n",
      "('+', b'+')\n",
      "('f', b'f')\n",
      "('*', b'*')\n",
      "('û', b'\\xc3\\xbb')\n",
      "('µ', b'\\xc2\\xb5')\n",
      "('d', b'd')\n",
      "('į', b'\\xc4\\xaf')\n",
      "('Ü', b'\\xc3\\x9c')\n",
      "('I', b'I')\n",
      "('ù', b'\\xc3\\xb9')\n",
      "('2', b'2')\n",
      "('Ē', b'\\xc4\\x92')\n",
      "('ý', b'\\xc3\\xbd')\n",
      "('¦', b'\\xc2\\xa6')\n"
     ]
    }
   ],
   "source": [
    "byte_level_alphabet = pre_tokenizers.\n",
    "byte_encoded_alphabet = [c.encode('utf-8') for c in byte_level_alphabet]\n",
    "print(len(byte_level_alphabet))\n",
    "for b, c in zip(byte_level_alphabet, byte_encoded_alphabet):\n",
    "    print((b,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xc4\\xa0D \\xc3\\x83 \\xc2\\xa5 \\xc4\\xa0 \\xc3\\x83 \\xc2\\xa4 r \\xc4\\xa0 \\xc3\\x83 \\xc2\\xb6 ar na \\xc4\\xa0d \\xc3\\x83 \\xc2\\xb6 d a \\xc4\\xa0sport \\xc3\\x83 \\xc2\\xa5 \\xc3\\x83 \\xc2\\xa4 \\xc3\\x83 \\xc2\\xb6 '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ĠD Ã ¥ Ġ Ã ¤ r Ġ Ã ¶ ar na Ġd Ã ¶ d a Ġsport Ã ¥ Ã ¤ Ã ¶ \".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ĠD Ã ¥ Ġ Ã ¤ r Ġ Ã ¶ ar na Ġd Ã ¶ d a "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
